{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clustering_from_kaggle_unrun.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IviD9U6bPAys",
        "colab_type": "text"
      },
      "source": [
        "Parse the text from the body of each document using Natural Language Processing (NLP).\n",
        "\n",
        "Turn each document instance  di  into a feature vector  Xi  using Term Frequency–inverse Document Frequency (TF-IDF).\n",
        "\n",
        "Apply Dimensionality Reduction to each feature vector  Xi  using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the two dimensional plane  X  embedding  Y1 .\n",
        "\n",
        "Use Principal Component Analysis (PCA) to project down the dimensions of  X  to a number of dimensions that will keep .95 variance while removing noise and outliers in embedding  Y2 .\n",
        "\n",
        "Apply k-means clustering on  Y2 , where  k  is 20, to label each cluster on  Y1 .\n",
        "\n",
        "Apply Topic Modeling on  X  using Latent Dirichlet Allocation (LDA) to discover keywords from each cluster.\n",
        "\n",
        "Investigate the clusters visually on the plot, zooming down to specific articles as needed, and via classification using Stochastic Gradient Descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18u86IEPBT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXZmxdgaP4VY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/edited_topics_all_news.csv')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI6ezUYVSYJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.head()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv5cWYciQNjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to handle duplicates\n",
        "df.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
        "df['abstract'].describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_r1hKfgQwN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to count words\n",
        "df['body_word_count'] = df['processed_content'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
        "\n",
        "df['body_unique_words']=df['processed_content'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
        "#df.head()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m47tQ2GoQwSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.to_csv('/content/drive/My Drive/all_news_with_count.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IsDy828QwPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.describe()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdqfw7aBQwML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Handling multiple languages\n",
        "#Next we are going to determine the language of each paper in the dataframe.\n",
        "#Not all of the sources are English and the language needs to be identified so that we know how handle these instances\n",
        "\n",
        "from tqdm import tqdm\n",
        "from langdetect import detect\n",
        "from langdetect import DetectorFactory\n",
        "\n",
        "# set seed\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# hold label - language\n",
        "languages = []\n",
        "\n",
        "# go through each text\n",
        "for ii in tqdm(range(0,len(df))):\n",
        "    # split by space into list, take the first x intex, join with space\n",
        "    text = df.iloc[ii]['body_text'].split(\" \")\n",
        "    \n",
        "    lang = \"en\"\n",
        "    try:\n",
        "        if len(text) > 50:\n",
        "            lang = detect(\" \".join(text[:50]))\n",
        "        elif len(text) > 0:\n",
        "            lang = detect(\" \".join(text[:len(text)]))\n",
        "    # ught... beginning of the document was not in a good format\n",
        "    except Exception as e:\n",
        "        all_words = set(text)\n",
        "        try:\n",
        "            lang = detect(\" \".join(all_words))\n",
        "        # what!! :( let's see if we can find any text in abstract...\n",
        "        except Exception as e:\n",
        "            \n",
        "            try:\n",
        "                # let's try to label it through the abstract then\n",
        "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
        "            except Exception as e:\n",
        "                lang = \"unknown\"\n",
        "                pass\n",
        "    \n",
        "    # get the language    \n",
        "    languages.append(lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_VnO4o_SuvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "languages_dict = {}\n",
        "for lang in set(languages):\n",
        "    languages_dict[lang] = languages.count(lang)\n",
        "    \n",
        "print(\"Total: {}\\n\".format(len(languages)))\n",
        "pprint(languages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbwQbFFdSu1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['language'] = languages\n",
        "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
        "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
        "plt.title(\"Distribution of Languages in Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU43q61oSuyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn3JAVh2Sy6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def vectorize(text, maxx_features):\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
        "    X = vectorizer.fit_transform(text)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-r2h8hLSy5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = df['processed_text'].values\n",
        "X = vectorize(text, 2 ** 12)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibLE_oKtTVzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec-YWkA-TWua",
        "colab_type": "text"
      },
      "source": [
        "PCA & Clustering\n",
        "Let's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you don’t destroy much of the information, but hopefully will remove some noise/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3dGeJITXqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_reduced= pca.fit_transform(X.toarray())\n",
        "X_reduced.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVljan0HTb00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S18UV9PQTg_P",
        "colab_type": "text"
      },
      "source": [
        "To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FunjOIhJTb4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MclSY1UMTq0i",
        "colab_type": "text"
      },
      "source": [
        "How many clusters?\n",
        "To find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-PXZ07jTbzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# run kmeans with many different k\n",
        "distortions = []\n",
        "K = range(2, 50)\n",
        "for k in K:\n",
        "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
        "    k_means.fit(X_reduced)\n",
        "    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
        "    #print('Found distortion for {} clusters'.format(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO2cz-cVTvES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_line = [K[0], K[-1]]\n",
        "Y_line = [distortions[0], distortions[-1]]\n",
        "\n",
        "# Plot the elbow\n",
        "plt.plot(K, distortions, 'b-')\n",
        "plt.plot(X_line, Y_line, 'r')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method showing the optimal k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQuCt230Tvlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 20\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X_reduced)\n",
        "df['y'] = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW47uWwfT8Bu",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality Reduction with t-SNE\n",
        "Using t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the body_text can be plotted.\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB7SBdWdTvpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n",
        "X_embedded = tsne.fit_transform(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7_pKPm1TvkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.color_palette(\"bright\", 1)\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
        "plt.title('t-SNE with no Labels')\n",
        "plt.savefig(\"t-sne_covid19.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxvcuRsOUKYW",
        "colab_type": "text"
      },
      "source": [
        "This looks pretty bland. There are some clusters we can immediately detect, but the many instances closer to the center are harder to separate. t-SNE did a good job at reducing the dimensionality, but now we need some labels. Let's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VlI7TQvTvDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.hls_palette(20, l=.4, s=.9)\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
        "plt.title('t-SNE with Kmeans Labels')\n",
        "plt.savefig(\"improved_cluster_tsne.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQ2mKuFUSlo",
        "colab_type": "text"
      },
      "source": [
        "The labeled plot gives better insight into how the papers are grouped. It is interesting that both k-means and t-SNE are able to agree on certain clusters even though they were ran independetly. The location of each paper on the plot was determined by t-SNE while the label (color) was determined by k-means. If we look at a particular part of the plot where t-SNE has grouped many articles forming a cluster, it is likely that k-means is uniform in the labeling of this cluster (most of the cluster is the same color). This behavior shows that structure within the literature can be observed and measured to some extent.\n",
        "\n",
        "Now there are other cases where the colored labels (k-means) are spread out on the plot (t-SNE). This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it hard to cleanly separate them. This effect can be observed in the formation of subclusters on the plot. These subclusters are a conglomeration of different k-means labels but may share some connection determined by t-SNE.\n",
        "\n",
        "This organization of the data does not act as a simple search engine. The clustering + dimensionality reduction is performed on the mathematical similarities of the publications. As an unsupervised approach, the algorithms may even find connections that were unnaparent to humans. This may highlight hidden shared information and advance further research."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBwYz7MIUC6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE_-OJapUDA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsVDvn-cUWgr",
        "colab_type": "text"
      },
      "source": [
        "Topic Modeling on Each Cluster\n",
        "Now we will attempt to find the most significant words in each clusters. K-means clustered the articles but did not label the topics. Through topic modeling we will find out what the most important terms for each cluster are. This will add more meaning to the cluster by giving keywords to quickly identify the themes of the cluster.\n",
        "\n",
        "For topic modeling, we will use LDA (Latent Dirichlet Allocation). In LDA, each document can be described by a distribution of topics and each topic can be described by a distribution of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xx8JkR7UC9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oe8ZGjxUC5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First we will create 20 vectorizers, one for each of our cluster labels\n",
        "\n",
        "vectorizers = []\n",
        "    \n",
        "for ii in range(0, 20):\n",
        "    # Creating a vectorizer\n",
        "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft_CV5qWUh7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizers[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0F7yLs7UiBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we will vectorize the data from each of our clusters\n",
        "\n",
        "vectorized_data = []\n",
        "\n",
        "for current_cluster, cvec in enumerate(vectorizers):\n",
        "    try:\n",
        "        vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'processed_text']))\n",
        "    except Exception as e:\n",
        "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
        "        vectorized_data.append(None)\n",
        "len(vectorized_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi_IEY9LUh-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #number of topics per cluster\n",
        "NUM_TOPICS_PER_CLUSTER = 20\n",
        "\n",
        "lda_models = []\n",
        "for ii in range(0, 20):\n",
        "    # Latent Dirichlet Allocation Model\n",
        "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
        "    lda_models.append(lda)\n",
        "    \n",
        "lda_models[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ARj3FuUh5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For each cluster, we had created a correspoding LDA model in the previous step. We will now fit_transform all the LDA models on their respective cluster vectors\n",
        "\n",
        "clusters_lda_data = []\n",
        "\n",
        "for current_cluster, lda in enumerate(lda_models):\n",
        "    # print(\"Current Cluster: \" + str(current_cluster))\n",
        "    \n",
        "    if vectorized_data[current_cluster] != None:\n",
        "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfegTA_GUuSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracts the keywords from each cluster\n",
        "\n",
        "# Functions for printing keywords for each topic\n",
        "def selected_topics(model, vectorizer, top_n=3):\n",
        "    current_words = []\n",
        "    keywords = []\n",
        "    \n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
        "        for word in words:\n",
        "            if word[0] not in current_words:\n",
        "                keywords.append(word)\n",
        "                current_words.append(word[0])\n",
        "                \n",
        "    keywords.sort(key = lambda x: x[1])  \n",
        "    keywords.reverse()\n",
        "    return_values = []\n",
        "    for ii in keywords:\n",
        "        return_values.append(ii[0])\n",
        "    return return_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io_MXWL4UuYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Append list of keywords for a single cluster to 2D list of length NUM_TOPICS_PER_CLUSTER\n",
        "\n",
        "all_keywords = []\n",
        "for current_vectorizer, lda in enumerate(lda_models):\n",
        "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
        "\n",
        "    if vectorized_data[current_vectorizer] != None:\n",
        "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))\n",
        "all_keywords[0][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErmseRzaUuWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(all_keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBxv6Ev-UuQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoAOdiT5VDd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save current outputs to file\n",
        "#Re-running some parts of the notebook (especially vectorization and t-SNE) are time intensive tasks. We want to make sure that the important outputs for generating the bokeh plot are saved for future use.\n",
        "\n",
        "f=open('topics.txt','w')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for ii in all_keywords:\n",
        "\n",
        "    if vectorized_data[count] != None:\n",
        "        f.write(', '.join(ii) + \"\\n\")\n",
        "    else:\n",
        "        f.write(\"Not enough instances to be determined. \\n\")\n",
        "        f.write(', '.join(ii) + \"\\n\")\n",
        "    count += 1\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOktdFQ3VDcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# save the COVID-19 DataFrame, too large for github\n",
        "pickle.dump(df, open(\"df_covid.p\", \"wb\" ))\n",
        "\n",
        "# save the final t-SNE\n",
        "pickle.dump(X_embedded, open(\"X_embedded.p\", \"wb\" ))\n",
        "\n",
        "# save the labels generate with k-means(20)\n",
        "pickle.dump(y_pred, open(\"y_pred.p\", \"wb\" ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGsyM2NnVIfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HVuXhNVVIjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHnH2LOZVMub",
        "colab_type": "text"
      },
      "source": [
        "Classify\n",
        "\n",
        "Though arbitrary, after running kmeans, the data is now 'labeled'. This means that we now use supervised learning to see how well the clustering generalizes. This is just one way to evaluate the clustering. If k-means was able to find a meaningful split in the data, it should be possible to train a classifier to predict which cluster a given instance should belong to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tQKAsnNVIdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to print out classification model report\n",
        "def classification_report(model_name, test, pred):\n",
        "    from sklearn.metrics import precision_score, recall_score\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.metrics import f1_score\n",
        "    \n",
        "    print(model_name, \":\\n\")\n",
        "    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n",
        "    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='macro')) * 100), \"%\")\n",
        "    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='macro')) * 100), \"%\")\n",
        "    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='macro')) * 100), \"%\")\n",
        "    \n",
        "#Let's split the data into train/test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# test set size of 20% of the data and the random seed 42 <3\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(),y_pred, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train size:\", len(X_train))\n",
        "print(\"X_test size:\", len(X_test), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLN3atfGVWD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CcZ86T4Varw",
        "colab_type": "text"
      },
      "source": [
        "Precision is ratio of True Positives to True Positives + False Positives. This is the accuracy of positive predictions\n",
        "Recall (also known as TPR) measures the ratio of True Positives to True Positives + False Negatives. It measures the ratio of positive instances that are correctly detected by the classifer.\n",
        "F1 score is the harmonic average of the precision and recall. F1 score will only be high if both precision and recall are high"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PAY_BZCVWMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# SGD instance\n",
        "sgd_clf = SGDClassifier(max_iter=10000, tol=1e-3, random_state=42, n_jobs=4)\n",
        "# train SGD\n",
        "sgd_clf.fit(X_train, y_train)\n",
        "\n",
        "# cross validation predictions\n",
        "sgd_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3, n_jobs=4)\n",
        "\n",
        "# print out the classification report\n",
        "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_train, sgd_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHfLAURNVWIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross validation predictions\n",
        "sgd_pred = cross_val_predict(sgd_clf, X_test, y_test, cv=3, n_jobs=4)\n",
        "\n",
        "# print out the classification report\n",
        "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_test, sgd_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C3b8fBtVWHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_cv_score = cross_val_score(sgd_clf, X.toarray(), y_pred, cv=10)\n",
        "print(\"Mean cv Score - SGD: {:,.3f}\".format(float(sgd_cv_score.mean()) * 100), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvtINQUVWCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bjrhgEVqu5",
        "colab_type": "text"
      },
      "source": [
        "Plotting the data\n",
        "\n",
        "The previous steps have given us clustering labels and a dataset of papers reduced to two dimensions. By pairing this with Bokeh, we can create an interactive plot of the literature. This should organize the papers such that related publications are in close proximity. To try to undertstand what the similarities may be, we have also performed topic modelling on each cluster of papers in order to pick out the key terms.\n",
        "\n",
        "Bokeh will pair the actual papers with their positions on the t-SNE plot. Through this approach it will be easier to see how papers fit together, allowing for both exploration of the dataset and evaluation of the clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3y-PyNVrIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# change into lib directory to load plot python scripts\n",
        "main_path = os.getcwd()\n",
        "lib_path = '/kaggle/input/kaggle-resources'\n",
        "os.chdir(lib_path)\n",
        "# required libraries for plot\n",
        "from call_backs import input_callback, selected_code  # file with customJS callbacks for bokeh\n",
        "                                                      # github.com/MaksimEkin/COVID19-Literature-Clustering/blob/master/lib/call_backs.py\n",
        "import bokeh\n",
        "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput\n",
        "from bokeh.palettes import Category20\n",
        "from bokeh.transform import linear_cmap, transform\n",
        "from bokeh.io import output_file, show, output_notebook\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\n",
        "from bokeh.layouts import column, widgetbox, row, layout\n",
        "from bokeh.layouts import column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFxdSP9AV8sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go back\n",
        "os.chdir(main_path)\n",
        "\n",
        "#Load the Keywords per Cluster\n",
        "import os\n",
        "\n",
        "topic_path = 'topics.txt'\n",
        "with open(topic_path) as f:\n",
        "    topics = f.readlines()\n",
        "\n",
        "# show on notebook\n",
        "output_notebook()\n",
        "# target labels\n",
        "y_labels = y_pred\n",
        "\n",
        "# data sources\n",
        "source = ColumnDataSource(data=dict(\n",
        "    x= X_embedded[:,0], \n",
        "    y= X_embedded[:,1],\n",
        "    x_backup = X_embedded[:,0],\n",
        "    y_backup = X_embedded[:,1],\n",
        "    desc= y_labels, \n",
        "    titles= df['title'],\n",
        "    authors = df['authors'],\n",
        "    journal = df['journal'],\n",
        "    abstract = df['abstract_summary'],\n",
        "    labels = [\"C-\" + str(x) for x in y_labels],\n",
        "    links = df['doi']\n",
        "    ))\n",
        "\n",
        "# hover over information\n",
        "hover = HoverTool(tooltips=[\n",
        "    (\"Title\", \"@titles{safe}\"),\n",
        "    (\"Author(s)\", \"@authors{safe}\"),\n",
        "    (\"Journal\", \"@journal\"),\n",
        "    (\"Abstract\", \"@abstract{safe}\"),\n",
        "    (\"Link\", \"@links\")\n",
        "],\n",
        "point_policy=\"follow_mouse\")\n",
        "\n",
        "# map colors\n",
        "mapper = linear_cmap(field_name='desc', \n",
        "                     palette=Category20[20],\n",
        "                     low=min(y_labels) ,high=max(y_labels))\n",
        "\n",
        "# prepare the figure\n",
        "plot = figure(plot_width=1200, plot_height=850, \n",
        "           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'tap'], \n",
        "           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\", \n",
        "           toolbar_location=\"above\")\n",
        "\n",
        "# plot settings\n",
        "plot.scatter('x', 'y', size=5, \n",
        "          source=source,\n",
        "          fill_color=mapper,\n",
        "          line_alpha=0.3,\n",
        "          line_color=\"black\",\n",
        "          legend = 'labels')\n",
        "plot.legend.background_fill_alpha = 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meXOpP-BV8q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Widgets\n",
        "# Keywords\n",
        "text_banner = Paragraph(text= 'Keywords: Slide to specific cluster to see the keywords.', height=45)\n",
        "input_callback_1 = input_callback(plot, source, text_banner, topics)\n",
        "\n",
        "# currently selected article\n",
        "div_curr = Div(text=\"\"\"Click on a plot to see the link to the article.\"\"\",height=150)\n",
        "callback_selected = CustomJS(args=dict(source=source, current_selection=div_curr), code=selected_code())\n",
        "taptool = plot.select(type=TapTool)\n",
        "taptool.callback = callback_selected\n",
        "\n",
        "# WIDGETS\n",
        "slider = Slider(start=0, end=20, value=20, step=1, title=\"Cluster #\", callback=input_callback_1)\n",
        "keyword = TextInput(title=\"Search:\", callback=input_callback_1)\n",
        "\n",
        "# pass call back arguments\n",
        "input_callback_1.args[\"text\"] = keyword\n",
        "input_callback_1.args[\"slider\"] = slider\n",
        "Style\n",
        "# STYLE\n",
        "slider.sizing_mode = \"stretch_width\"\n",
        "slider.margin=15\n",
        "\n",
        "keyword.sizing_mode = \"scale_both\"\n",
        "keyword.margin=15\n",
        "\n",
        "div_curr.style={'color': '#BF0A30', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\n",
        "div_curr.sizing_mode = \"scale_both\"\n",
        "div_curr.margin = 20\n",
        "\n",
        "text_banner.style={'color': '#0269A4', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\n",
        "text_banner.sizing_mode = \"scale_both\"\n",
        "text_banner.margin = 20\n",
        "\n",
        "plot.sizing_mode = \"scale_both\"\n",
        "plot.margin = 5\n",
        "\n",
        "r = row(div_curr,text_banner)\n",
        "r.sizing_mode = \"stretch_width\"\n",
        "#SHOW\n",
        "# LAYOUT OF THE PAGE\n",
        "l = layout([\n",
        "    [slider, keyword],\n",
        "    [text_banner],\n",
        "    [div_curr],\n",
        "    [plot],\n",
        "])\n",
        "l.sizing_mode = \"scale_both\"\n",
        "\n",
        "# show\n",
        "output_file('t-sne_covid-19_interactive.html')\n",
        "show(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGFakF5YV4Ei",
        "colab_type": "text"
      },
      "source": [
        "Conclusion\n",
        "\n",
        "In this project, we have attempted to cluster published literature on COVID-19 and reduce the dimensionality of the dataset for visualization purposes. This has allowed for an interactive scatter plot of papers related to COVID-19, in which material of similar theme is grouped together. Grouping the literature in this way allows for professionals to quickly find material related to a central topic. Instead of having to manually search for related work, every publication is connected to a larger topic cluster. The clustering of the data was done through k-means on a pre-processed, vectorized version of the literature’s body text. As k-means simply split the data into clusters, topic modeling through LDA was performed to identify keywords. This gave the topics that were prevalent in each of the clusters. Both the clusters and keywords are found through unsupervised learning models and can be useful in revealing patterns that humans may not have even thought about. In no part of this project did we have to manually organize the papers: the results are due to latent connections in the data.\n",
        "\n",
        "K-means (represented by colors) and t-SNE (represented by points) were able to independently find clusters, showing that relationships between papers can be identified and measured. Papers written on highly similar topics are typically near each other on the plot and bear the same k-means label. However, due to the complexity of the dataset, k-means and t-SNE will sometimes arrive at different decisions. The topics of much of the given literature are continuous and will not have a concrete decision boundary. This means that k-means and t-SNE can find different similarities to group the papers by. In these conditions, our approach performs quite well.\n",
        "\n",
        "As this is an unsupervised learning problem, the evaluation of our work was not an exact science. First, the plot was examined to assert that clusters were actually being formed. After being convinced of this, we examined the titles/abstracts of some of the papers in different clusters. For the most part, similar research areas were clustered. Our last evaluation method was classification. By training a classification model with the k-means labels and then testing it on a separate subset of the data, we could see that the clustering was not completely arbitrary as the classifier performed well.\n",
        "\n",
        "Our manual inspection of the documents was quite limited, as neither of the authors are qualified to assess the meaning of the literature. Even so, it was apparent that articles on key topics could be easily found in close proximity to each other. For example, searching for 'mask' can reveal a sub cluster of papers that evaluate the efficacy of masks. We believe that health professionals can use this tool to find real links in the texts. By organizing the literature, qualified people can quickly find related publications that answer the task questions. This project can further be improved by abstracting the underlying data analysis techniques as described in this notebook to develop a user interface/tool that presents the related articles in a user-friendly manner.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I7F4rs-V4_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}